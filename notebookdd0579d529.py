# -*- coding: utf-8 -*-
"""notebookdd0579d529

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebookdd0579d529-2e3f1284-0150-4a27-a3f2-25809eed4e1d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240626/auto/storage/goog4_request%26X-Goog-Date%3D20240626T131435Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8a9f46d33ca4be6525e85c91dc43fc86470d74539d07968be218c0dd7687748a036df1ee2250ad97063f0312a08409226ce2cc8ff648d12a8193edb5d5f876051f5ece1815cf2feb2d26cf5a6190d704ae9b14ff13d4ace945120e98fd178bcb37a19af50b3924c07ec021a4249b2ecbccfee99cc17ae8788f8e9e87e3aa5b4d98bd9bb597a90a467e06add0ea9315c07d6d3aa806b172f54b285a1b833b47931b2526b6dab3bf1babe8358eb54355bcf312a2d415dd88c1bc4e366e7c4122b44c6ea784ae8c2bbc3c314c32add3fb1a0a8c93b789c0cbdb79eff571aa95c5254b2740b9b0281e2a2c5a2c1bc2be2a68a605bda9d4cfefacbc8c7030271d5a88
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load
from __future__ import print_function
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# 1. Classification Using Convolutional Neural Networks:

## (a) Importing Important Libraries
"""

# Commented out IPython magic to ensure Python compatibility.

import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.regularizers import l2
import os
from random import randint
from PIL import Image
import cv2
import matplotlib.pyplot as plt
from tqdm import tqdm
import matplotlib.pyplot as plt
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('fivethirtyeight')
# %matplotlib inline

# Defining the parameters
batch_size = 32
num_classes = 10
epochs = 50

"""## (b) Defining folder paths"""

folder_path = r"C:\downloads\Covid19-dataset"

train_covid = folder_path + r"\train\Covid"
train_normal = folder_path + r"\train\Normal"
train_pneumonia = folder_path + r"\train\Viral Pneumonia"

test_covid = folder_path + r"\test\Covid"
test_normal = folder_path + r"\test\Normal"
test_pneumonia = folder_path + r"\test\Viral Pneumonia"

len(os.listdir(train_covid))

"""## (c) Defining train and test image sets:"""

covid_train = []
normal_train = []
pneumonia_train = []
covid_test = []
normal_test = []
pneumonia_test = []

covid_label = np.full(len(os.listdir(train_covid)),2)

for fname in tqdm(os.listdir(train_covid)):
    img_path = os.path.join(train_covid, fname)
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)
    img = img / 255.0
    covid_train.append(img)

covid_train[-1]

normal_label = np.zeros(len(os.listdir(train_normal)))

for fname in tqdm(os.listdir(train_normal)):
    img_path = train_normal + "\\" + fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation = cv2.INTER_AREA)
    img = img / 255
    normal_train.append(img)

pneumonia_label = np.ones(len(os.listdir(train_pneumonia)))

for fname in tqdm(os.listdir(train_pneumonia)):
    img_path = train_pneumonia + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation = cv2.INTER_AREA)
    img = img / 255
    pneumonia_train.append(img)

# Covid
covid_test = []
covid_label_test = np.full(len(os.listdir(test_covid)),2)

for fname in tqdm(os.listdir(test_covid)):
    img_path = test_covid + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation = cv2.INTER_AREA)
    img = img / 255
    covid_test.append(img)

#Normal
normal_test = []
normal_label_test = np.zeros(len(os.listdir(test_normal)))

for fname in tqdm(os.listdir(test_normal)):
    img_path = test_normal + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation = cv2.INTER_AREA)
    img = img / 255
    normal_test.append(img)

pneumonia_test = []
pneumonia_label_test = np.ones(len(os.listdir(test_pneumonia)))

for fname in tqdm(os.listdir(test_pneumonia)):
    img_path = test_pneumonia + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (50, 50), interpolation = cv2.INTER_AREA)
    img = img / 255
    pneumonia_test.append(img)

X_test_images = np.concatenate((normal_test, pneumonia_test,covid_test))*255

fig, ax = plt.subplots(2, 3, figsize = (10, 8))

image_path = os.path.join(train_covid, os.listdir(train_covid)[0])
img0 = cv2.imread(image_path)
ax[0][0].imshow(img0)
ax[0][0].set_title("Covid-19")

image_path = os.path.join(train_normal, os.listdir(train_normal)[0])
img1 = cv2.imread(image_path)
ax[0][1].imshow(img1)
ax[0][1].set_title("Normal")

image_path = os.path.join(train_pneumonia, os.listdir(train_pneumonia)[0])
img2 = cv2.imread(image_path)
ax[0][2].imshow(img2)
ax[0][2].set_title("Pneumonia")


image_path = os.path.join(test_covid, os.listdir(test_covid)[0])
img3 = cv2.imread(image_path)
ax[1][0].imshow(img3)
ax[1][0].set_title("Covid-19")

image_path = os.path.join(test_normal, os.listdir(test_normal)[0])
img4 = cv2.imread(image_path)
ax[1][1].imshow(img4)
ax[1][1].set_title("Normal")

image_path = os.path.join(test_pneumonia, os.listdir(test_pneumonia)[0])
img5 = cv2.imread(image_path)
ax[1][2].imshow(img5)
ax[1][2].set_title("Pneumonia")

plt.show()

X_train = np.concatenate(( normal_train, pneumonia_train,covid_train))
y_train = np.concatenate(( normal_label, pneumonia_label,covid_label))

X_test = np.concatenate(( normal_test, pneumonia_test,covid_test))
y_test = np.concatenate(( normal_label_test, pneumonia_label_test,covid_label_test))

print(len(covid_train))
print(len(normal_train))
print(len(pneumonia_train))

print(len(X_train))

len(X_train) == len(covid_train) + len(normal_train) + len(pneumonia_train)

assert len(X_train) == len(covid_train) + len(normal_train) + len(pneumonia_train)

from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D
np.random.seed(42)

import tensorflow as tf
tf.random.set_seed(42)

"""##  (d) Defining the Convolutional model:"""

model = Sequential()

model.add(Conv2D(filters = 16,
                 kernel_size = (5, 5),
                 activation = "relu",
                 input_shape = (50, 50, 3)))


model.add(MaxPool2D(pool_size = (2, 2)))

model.add(Conv2D(filters = 16,
                 kernel_size = (3, 3),
                 activation = "relu"))

model.add(MaxPool2D(pool_size = (2, 2)))

model.add(Flatten())

model.add(Dense(256, activation = "relu"))

model.add(Dense(3, activation = "softmax"))

model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"])

history = model.fit(X_train,
                    y_train,
                    epochs = 50,
                    validation_split = 0.2)

"""## (e) Accuracy and Validation plots:"""

def plot_learning_curve(history):

    loss = history.history['loss']
    val_loss = history.history['val_loss']
    plt.plot(history.history["accuracy"], label = "accuracy")
    plt.plot(history.history["val_accuracy"], label = "val_accuracy")
    plt.legend();
    plt.figure();

    plt.plot( loss, label='training loss')
    plt.plot( val_loss, label='validation loss')
    plt.legend();

plot_learning_curve(history)

"""## (f) Making predictions on test sets:"""

y_pred = model.predict(X_test)

prediction = np.argmax(y_pred, axis = 1)
prediction

"""## (g) Performance matrix:"""

from sklearn.metrics import classification_report
print(classification_report(prediction, y_test))

pred_list = X_test_images
len(pred_list)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
img_gen = ImageDataGenerator(rescale=1./255)

"""## (e) Choosing random 10 images and predicting their classes:"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
from random import randint

pred_list = X_test_images
img_gen = ImageDataGenerator(rescale=1./255)

if len(pred_list) == 0:
    print("Error: Empty array.")
else:
    for i in range(10):
        if len(pred_list) > 0:
            num = randint(0, len(pred_list) - 1)
            img = Image.fromarray(pred_list[num].astype(np.uint8))  # Convert data type to uint8
            img = img.resize((50, 50))
            display(img)
            img_arr = np.array(img)
            img_arr = img_arr[np.newaxis, :]
            img_arr = img_arr.astype("float")
            img_arr = img_gen.standardize(img_arr)
            probability = model(img_arr)
            pred = np.argmax(probability)
            # print("Model Prediction: ", idx_to_classes[pred])
            print("Actual label: ", int(y_test[num]))
            print("Model Prediction: ", pred)
            print("==========================================")
        else:
            print("Error: Empty array.")

"""# 2. Using Transfer Learning (VGG16 model):"""

#import libraries
from IPython import display
import os
import math
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random as python_random
import tensorflow as tf
import seaborn as sns
import math

#define VGG16 Model
from tensorflow.keras.layers.experimental import preprocessing
height, width = 224, 224

tf.keras.backend.clear_session()
input_shape = (height, width, 3)
base_model = tf.keras.applications.vgg16.VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=input_shape
)
base_model.trainable = False

model_vgg16 = tf.keras.Sequential()
model_vgg16.add(base_model)
model_vgg16.add(tf.keras.layers.GlobalAveragePooling2D())

model_vgg16.add(tf.keras.layers.Flatten())
model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))
model_vgg16.add(tf.keras.layers.Dropout(0.5))
model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))
model_vgg16.add(tf.keras.layers.Dropout(0.5))

model_vgg16.add(tf.keras.layers.Dense(3, activation='softmax'))

model_vgg16.compile(loss='SparseCategoricalCrossentropy',
              optimizer=tf.keras.optimizers.Adam(0.001),
              metrics=['acc'])
model_vgg16.summary()

# ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
from PIL import ImageFilter

height, width = 224, 224
batch_size=64


def generate_data(DIR):
    datagen = ImageDataGenerator(rescale=1./255.)

    generator = datagen.flow_from_directory(
        DIR,
        batch_size=batch_size,
        shuffle=True,
        seed=42,
        class_mode='binary',
        target_size=(height, width),
        classes={'Normal': 0, 'Viral Pneumonia': 1,'Covid': 2}
    )
    return generator

TRAINING_DIR = r"C:\downloads\Covid19-dataset\train"
TESTING_DIR = r"C:\downloads\Covid19-dataset\test"

train_generator = generate_data(TRAINING_DIR)
test_generator = generate_data(TESTING_DIR)

total_image = np.concatenate([train_generator.labels,test_generator.labels])

print('\n\n',{'Normal_cases':len(np.where(total_image==0)[0]),
      'Viral_Pneumonia_cases':len(np.where(total_image==1)[0]),
             'Covid_cases':len(np.where(total_image==2)[0])})

"""## (a) Training the model:"""

checkpoint = tf.keras.callbacks.ModelCheckpoint('model/vgg16_best.h5', monitor='acc', verbose=1, mode='max',save_best_only=True)
early = tf.keras.callbacks.EarlyStopping(monitor="val_loss", mode="min",restore_best_weights=True, patience=5)

callbacks_list = [checkpoint,early]

history = model_vgg16.fit(
        train_generator,
        validation_data = test_generator,
        #steps_per_epoch=10,
        epochs=50,
        shuffle=False,
        verbose=True,
        callbacks=callbacks_list)

history.save("keras_model.h5")

"""## (b) Accuracy and Validation plots:"""

def plot_learning_curve(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(acc))

    plt.plot(epochs, acc, label='training acc')
    plt.plot(epochs, val_acc, label='validation acc')
    plt.legend();
    plt.figure();

    plt.plot(epochs, loss, label='training loss')
    plt.plot(epochs, val_loss, label='validation loss')
    plt.legend();

plot_learning_curve(history)

#get results
train_result = model_vgg16.evaluate(train_generator)
test_result = model_vgg16.evaluate(test_generator)

no_augmented_df = pd.DataFrame(zip(train_result,test_result),columns=['Train','Val'],index=['Loss','Acc'])
no_augmented_df

"""## (c) Preparing compatible arrays to feed the model for verifying its predictions:"""

covid_test_vgg = []
normal_test_vgg = []
pneumonia_test_vgg = []

covid_label_test_vgg = np.full(len(os.listdir(test_covid)),2)

for fname in tqdm(os.listdir(test_covid)):
    img_path = test_covid + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (244,244), interpolation = cv2.INTER_AREA)
    #img = img / 255
    covid_test_vgg.append(img)

normal_label_test_vgg = np.zeros(len(os.listdir(test_normal)))

for fname in tqdm(os.listdir(test_normal)):
    img_path = test_normal + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (244,244), interpolation = cv2.INTER_AREA)
    #img = img / 255
    normal_test_vgg.append(img)

pneumonia_label_test_vgg = np.ones(len(os.listdir(test_pneumonia)))

for fname in tqdm(os.listdir(test_pneumonia)):
    img_path = test_pneumonia + "\\"+ fname
    img = cv2.imread(img_path)
    img = cv2.resize(img, (244,244), interpolation = cv2.INTER_AREA)
    #img = img / 255
    pneumonia_test_vgg.append(img)

# Convert the lists to NumPy arrays
covid_test = np.array(covid_test)
normal_test = np.array(normal_test)
pneumonia_test = np.array(pneumonia_test)

# Check the shapes of the arrays
print(covid_test.shape)
print(normal_test.shape)
print(pneumonia_test.shape)

X_test = np.concatenate(( normal_test_vgg, pneumonia_test_vgg,covid_test_vgg))
y_test = np.concatenate(( normal_label_test_vgg, pneumonia_label_test_vgg,covid_label_test_vgg))

from tensorflow.keras.models import load_model

#  model_vgg16 = load_model(r"C:\Users\rajes\Downloads\model\vgg16_best.h5")

from PIL import Image
import numpy as np

# Assuming X_test contains your input images with shape (None, 50, 50, 3)
y_pred=[]
img_gen = ImageDataGenerator(rescale=1./255.)
resized_images = []
for image in X_test:
    # Convert the image array to PIL Image

    img = Image.fromarray(image.astype(np.uint8))  # Convert data type to uint8
    img = img.resize((224, 224))
    #display(img)


    img_arr = np.array(img)
    img_arr = img_arr[np.newaxis, :]

    #print(img_arr)
    img_arr = img_arr.astype("float")
    img_arr = img_gen.standardize(img_arr)
    probability = model_vgg16(img_arr)
    #print(num)
    pred = np.argmax(probability)
    y_pred.append(pred)

# Convert the resized images list to a numpy array
#resized_X_test = np.array(resized_images)

# Now you can pass the resized_X_test to your model for prediction
#y_pred = model_vgg16.predict(resized_X_test)

y_pred
y_pred = np.reshape(y_pred, (1, -1))

y_pred[0]

"""## (d) Perfomance Matrices:"""

from sklearn.metrics import classification_report
print(classification_report(y_pred[0], y_test))

"""## (e) Choosing random 10 images and verifying model's predictions:"""

from IPython.display import display
pred_list = X_test
img_gen = ImageDataGenerator(rescale=1./255.)

if len(pred_list) == 0:
    print("Error: Empty array.")
else:
    for i in range(10):
        if len(pred_list) > 0:
            num = randint(0, len(pred_list) - 1)
            img = Image.fromarray(pred_list[num].astype(np.uint8))  # Convert data type to uint8
            img = img.resize((224, 224))
            display(img)
            img = np.expand_dims(img, axis=0)  # Add an extra dimension for the batch size
            img= img.reshape((1, 224, 224, 3))

            img_arr = np.array(img)
            #img_arr = img_arr[np.newaxis, :]
            img_arr=img_arr
            #print(img_arr)
            img_arr = img_arr.astype("float")
            img_arr = img_gen.standardize(img_arr)
            probability = model_vgg16(img_arr)
            print(num)
            pred = np.argmax(probability)
            # print("Model Prediction: ", idx_to_classes[pred])
            print("Actual label: ", int(y_test[num]))
            print("Model Prediction: ", pred)
            print("==========================================")
        else:
            print("Error: Empty array.")

"""# 3.Conclusion

**Case 1:**  A model with 2 conv2D layers trained afresh on the training image dataset

**Case 2:**  VGG16 model which has been pretrained on ImageNet dataset of 1.2 million images and its weights has been freezed, some more layers were added to this model and those layers were fine tuned for the current image dataset to ensure effective transfer learning.  

**In Case 1:**

Accuracy: 0.86,
Precision (weighted avg): 0.87,
Recall (weighted avg): 0.86,
F1-score (weighted avg): 0.86


**In **Case 2:****

Accuracy: 0.92,
Precision (weighted avg): 0.94,
Recall (weighted avg): 0.92,
F1-score (weighted avg): 0.93

Based on these metrics, we can see that Case 2 is performing better overall. It has a higher accuracy, precision, recall, and F1-score compared to Case 1. The macro average and weighted average metrics are also higher for Case 2.

Here's a breakdown of the comparison:

1. Accuracy: Case 2 has a higher accuracy, indicating that it has a higher proportion of correctly classified instances.
2. Precision: Case 2 has higher precision values for all classes, indicating a lower false positive rate and better ability to correctly identify true positives.
3. Recall: Case 2 has higher recall values for all classes, indicating a lower false negative rate and better ability to correctly identify all positive instances.
4. F1-score: Case 2 has a higher F1-score, which is the harmonic mean of precision and recall, indicating a better balance between precision and recall.

**Therefore, based on the provided metrics, Case 2 is performing better than Case 1. It achieves higher accuracy, precision, recall, and F1-score values, suggesting better overall performance in classification tasks.**
"""